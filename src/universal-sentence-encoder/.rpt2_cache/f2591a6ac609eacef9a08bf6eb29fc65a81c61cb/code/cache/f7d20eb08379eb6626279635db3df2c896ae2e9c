{"code":"/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport * as tslib_1 from \"tslib\";\r\n/**\r\n * Tokenizer.encode() is a port of `EncodeAsIds` from the SentencePiece library\r\n * (https://github.com/google/sentencepiece). Encode uses the Viterbi algorithm\r\n * to find the most likely sequence of tokens that comprise the input. For more\r\n * details, refer to https://arxiv.org/pdf/1804.10959.pdf.\r\n */\r\nimport * as tf from '@tensorflow/tfjs-core';\r\nimport { stringToChars } from '../util';\r\nimport { Trie } from './trie';\r\nvar separator = '\\u2581'; // This is the unicode character 'lower one eighth block'.\r\nfunction processInput(str) {\r\n    var normalized = str.normalize('NFKC');\r\n    return normalized.length > 0 ?\r\n        separator + normalized.replace(/ /g, separator) :\r\n        normalized;\r\n}\r\n// The first tokens are reserved for unk, control symbols, and user-defined\r\n// symbols.\r\nvar RESERVED_SYMBOLS_COUNT = 6;\r\nvar Tokenizer = /** @class */ (function () {\r\n    function Tokenizer(vocabulary, reservedSymbolsCount) {\r\n        if (reservedSymbolsCount === void 0) { reservedSymbolsCount = RESERVED_SYMBOLS_COUNT; }\r\n        this.vocabulary = vocabulary;\r\n        this.reservedSymbolsCount = reservedSymbolsCount;\r\n        this.trie = new Trie();\r\n        for (var i = this.reservedSymbolsCount; i < this.vocabulary.length; i++) {\r\n            this.trie.insert(this.vocabulary[i][0], this.vocabulary[i][1], i);\r\n        }\r\n    }\r\n    Tokenizer.prototype.encode = function (input) {\r\n        var nodes = [];\r\n        var words = [];\r\n        var best = [];\r\n        input = processInput(input);\r\n        var symbols = stringToChars(input);\r\n        for (var i = 0; i <= symbols.length; i++) {\r\n            nodes.push({});\r\n            words.push(0);\r\n            best.push(0);\r\n        }\r\n        // Construct the lattice.\r\n        for (var i = 0; i < symbols.length; i++) {\r\n            var matches = this.trie.commonPrefixSearch(symbols.slice(i));\r\n            for (var j = 0; j < matches.length; j++) {\r\n                var piece = matches[j];\r\n                var obj = { key: piece[0], score: piece[1], index: piece[2] };\r\n                var endPos = piece[0].length;\r\n                if (nodes[i + endPos][i] == null) {\r\n                    nodes[i + endPos][i] = [];\r\n                }\r\n                nodes[i + endPos][i].push(obj);\r\n            }\r\n        }\r\n        for (var endPos = 0; endPos <= symbols.length; endPos++) {\r\n            for (var startPos in nodes[endPos]) {\r\n                var arr = nodes[endPos][startPos];\r\n                for (var j = 0; j < arr.length; j++) {\r\n                    var word = arr[j];\r\n                    var score = word.score + best[endPos - word.key.length];\r\n                    if (best[endPos] === 0 || score >= best[endPos]) {\r\n                        best[endPos] = score;\r\n                        words[endPos] = arr[j].index;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        var results = [];\r\n        // Backward pass.\r\n        var iter = words.length - 1;\r\n        while (iter > 0) {\r\n            results.push(words[iter]);\r\n            iter -= this.vocabulary[words[iter]][0].length;\r\n        }\r\n        // Merge consecutive unks.\r\n        var merged = [];\r\n        var isPreviousUnk = false;\r\n        for (var i = 0; i < results.length; i++) {\r\n            var id = results[i];\r\n            if (!(isPreviousUnk && id === 0)) {\r\n                merged.push(id);\r\n            }\r\n            isPreviousUnk = id === 0;\r\n        }\r\n        return merged.reverse();\r\n    };\r\n    return Tokenizer;\r\n}());\r\nexport { Tokenizer };\r\n/**\r\n * Load the Tokenizer for use independently from the UniversalSentenceEncoder.\r\n *\r\n * @param pathToVocabulary (optional) Provide a path to the vocabulary file.\r\n */\r\nexport function loadTokenizer(pathToVocabulary) {\r\n    return tslib_1.__awaiter(this, void 0, void 0, function () {\r\n        var vocabulary, tokenizer;\r\n        return tslib_1.__generator(this, function (_a) {\r\n            switch (_a.label) {\r\n                case 0: return [4 /*yield*/, loadVocabulary(pathToVocabulary)];\r\n                case 1:\r\n                    vocabulary = _a.sent();\r\n                    tokenizer = new Tokenizer(vocabulary);\r\n                    return [2 /*return*/, tokenizer];\r\n            }\r\n        });\r\n    });\r\n}\r\n/**\r\n * Load a vocabulary for the Tokenizer.\r\n *\r\n * @param pathToVocabulary Defaults to the path to the 8k vocabulary used by the\r\n * UniversalSentenceEncoder.\r\n */\r\nexport function loadVocabulary(pathToVocabulary) {\r\n    return tslib_1.__awaiter(this, void 0, void 0, function () {\r\n        var vocabulary;\r\n        return tslib_1.__generator(this, function (_a) {\r\n            switch (_a.label) {\r\n                case 0: return [4 /*yield*/, tf.util.fetch(pathToVocabulary)];\r\n                case 1:\r\n                    vocabulary = _a.sent();\r\n                    return [2 /*return*/, vocabulary.json()];\r\n            }\r\n        });\r\n    });\r\n}\r\n//# sourceMappingURL=index.js.map","map":"{\"version\":3,\"file\":\"index.js\",\"sourceRoot\":\"\",\"sources\":[\"../src/tokenizer/index.ts\"],\"names\":[],\"mappings\":\"AAAA;;;;;;;;;;;;;;;GAeG;;AAEH;;;;;GAKG;AAEH,OAAO,KAAK,EAAE,MAAM,uBAAuB,CAAC;AAE5C,OAAO,EAAC,aAAa,EAAC,MAAM,SAAS,CAAC;AAEtC,OAAO,EAAC,IAAI,EAAC,MAAM,QAAQ,CAAC;AAE5B,IAAM,SAAS,GACX,QAAQ,CAAC,CAAE,0DAA0D;AAEzE,SAAS,YAAY,CAAC,GAAW;IAC/B,IAAM,UAAU,GAAG,GAAG,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC;IACzC,OAAO,UAAU,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;QAC1B,SAAS,GAAG,UAAU,CAAC,OAAO,CAAC,IAAI,EAAE,SAAS,CAAC,CAAC,CAAC;QACjD,UAAU,CAAC;AACjB,CAAC;AAED,2EAA2E;AAC3E,WAAW;AACX,IAAM,sBAAsB,GAAG,CAAC,CAAC;AAUjC;IAGE,mBACY,UAAsB,EACtB,oBAA6C;QAA7C,qCAAA,EAAA,6CAA6C;QAD7C,eAAU,GAAV,UAAU,CAAY;QACtB,yBAAoB,GAApB,oBAAoB,CAAyB;QACvD,IAAI,CAAC,IAAI,GAAG,IAAI,IAAI,EAAE,CAAC;QAEvB,KAAK,IAAI,CAAC,GAAG,IAAI,CAAC,oBAAoB,EAAE,CAAC,GAAG,IAAI,CAAC,UAAU,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACvE,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;SACnE;IACH,CAAC;IAED,0BAAM,GAAN,UAAO,KAAa;QAClB,IAAM,KAAK,GAAsC,EAAE,CAAC;QACpD,IAAM,KAAK,GAAa,EAAE,CAAC;QAC3B,IAAM,IAAI,GAAa,EAAE,CAAC;QAE1B,KAAK,GAAG,YAAY,CAAC,KAAK,CAAC,CAAC;QAE5B,IAAM,OAAO,GAAG,aAAa,CAAC,KAAK,CAAC,CAAC;QAErC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACxC,KAAK,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;YACf,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACd,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;SACd;QAED,yBAAyB;QACzB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACvC,IAAM,OAAO,GAAG,IAAI,CAAC,IAAI,CAAC,kBAAkB,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;YAE/D,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;gBACvC,IAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;gBACzB,IAAM,GAAG,GAAG,EAAC,GAAG,EAAE,KAAK,CAAC,CAAC,CAAC,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,EAAE,KAAK,EAAE,KAAK,CAAC,CAAC,CAAC,EAAC,CAAC;gBAE9D,IAAM,MAAM,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC;gBAC/B,IAAI,KAAK,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;oBAChC,KAAK,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,EAAE,CAAC;iBAC3B;gBAED,KAAK,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;aAChC;SACF;QAED,KAAK,IAAI,MAAM,GAAG,CAAC,EAAE,MAAM,IAAI,OAAO,CAAC,MAAM,EAAE,MAAM,EAAE,EAAE;YACvD,KAAK,IAAM,QAAQ,IAAI,KAAK,CAAC,MAAM,CAAC,EAAE;gBACpC,IAAM,GAAG,GAAG,KAAK,CAAC,MAAM,CAAC,CAAC,QAAQ,CAAC,CAAC;gBAEpC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,GAAG,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;oBACnC,IAAM,IAAI,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC;oBACpB,IAAM,KAAK,GAAG,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,CAAC;oBAE1D,IAAI,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,IAAI,KAAK,IAAI,IAAI,CAAC,MAAM,CAAC,EAAE;wBAC/C,IAAI,CAAC,MAAM,CAAC,GAAG,KAAK,CAAC;wBACrB,KAAK,CAAC,MAAM,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;qBAC9B;iBACF;aACF;SACF;QAED,IAAM,OAAO,GAAa,EAAE,CAAC;QAE7B,iBAAiB;QACjB,IAAI,IAAI,GAAG,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;QAC5B,OAAO,IAAI,GAAG,CAAC,EAAE;YACf,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC;YAC1B,IAAI,IAAI,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC;SAChD;QAED,0BAA0B;QAC1B,IAAM,MAAM,GAAG,EAAE,CAAC;QAClB,IAAI,aAAa,GAAG,KAAK,CAAC;QAC1B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;YACvC,IAAM,EAAE,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;YACtB,IAAI,CAAC,CAAC,aAAa,IAAI,EAAE,KAAK,CAAC,CAAC,EAAE;gBAChC,MAAM,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;aACjB;YAED,aAAa,GAAG,EAAE,KAAK,CAAC,CAAC;SAC1B;QAED,OAAO,MAAM,CAAC,OAAO,EAAE,CAAC;IAC1B,CAAC;IACH,gBAAC;AAAD,CAAC,AApFD,IAoFC;;AAED;;;;GAIG;AACH,MAAM,UAAgB,aAAa,CAAC,gBAAyB;;;;;wBACxC,qBAAM,cAAc,CAAC,gBAAgB,CAAC,EAAA;;oBAAnD,UAAU,GAAG,SAAsC;oBACnD,SAAS,GAAG,IAAI,SAAS,CAAC,UAAU,CAAC,CAAC;oBAC5C,sBAAO,SAAS,EAAC;;;;CAClB;AAED;;;;;GAKG;AACH,MAAM,UAAgB,cAAc,CAAC,gBAAwB;;;;;wBACxC,qBAAM,EAAE,CAAC,IAAI,CAAC,KAAK,CAAC,gBAAgB,CAAC,EAAA;;oBAAlD,UAAU,GAAG,SAAqC;oBACxD,sBAAO,UAAU,CAAC,IAAI,EAAE,EAAC;;;;CAC1B\"}","dts":{"name":"/Users/igorstojanov/code/evaluations/tfjs-models/universal-sentence-encoder/tokenizer/index.d.ts","writeByteOrderMark":false,"text":"/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { Trie } from './trie';\r\nexport declare type Vocabulary = Array<[string, number]>;\r\nexport declare class Tokenizer {\r\n    private vocabulary;\r\n    private reservedSymbolsCount;\r\n    trie: Trie;\r\n    constructor(vocabulary: Vocabulary, reservedSymbolsCount?: number);\r\n    encode(input: string): number[];\r\n}\r\n/**\r\n * Load the Tokenizer for use independently from the UniversalSentenceEncoder.\r\n *\r\n * @param pathToVocabulary (optional) Provide a path to the vocabulary file.\r\n */\r\nexport declare function loadTokenizer(pathToVocabulary?: string): Promise<Tokenizer>;\r\n/**\r\n * Load a vocabulary for the Tokenizer.\r\n *\r\n * @param pathToVocabulary Defaults to the path to the 8k vocabulary used by the\r\n * UniversalSentenceEncoder.\r\n */\r\nexport declare function loadVocabulary(pathToVocabulary: string): Promise<any>;\r\n"}}
